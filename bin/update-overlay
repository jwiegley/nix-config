#!/usr/bin/env python3
"""
update-overlay - Update Nix overlay package definitions to their latest versions.

Usage:
    update-overlay <package1> [package2] ...

Examples:
    update-overlay llama-swap llama-cpp
    update-overlay hfdownloader
    update-overlay --dry-run mlx-lm
"""

import argparse
import json
import os
import re
import subprocess
import sys
import tempfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional


# ANSI color codes
class Colors:
    RESET = "\033[0m"
    BOLD = "\033[1m"
    DIM = "\033[2m"
    RED = "\033[31m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"
    BLUE = "\033[34m"
    MAGENTA = "\033[35m"
    CYAN = "\033[36m"

    @classmethod
    def disable(cls):
        """Disable colors (e.g., when not a TTY)."""
        cls.RESET = cls.BOLD = cls.DIM = ""
        cls.RED = cls.GREEN = cls.YELLOW = cls.BLUE = cls.MAGENTA = cls.CYAN = ""


# Disable colors if not a TTY
if not sys.stdout.isatty():
    Colors.disable()


@dataclass
class PackageInfo:
    """Information about a package in an overlay."""
    name: str
    file_path: Path
    owner: str = ""
    repo: str = ""
    version: str = ""
    version_prefix: str = ""  # e.g., "v", "b", ""
    src_hash: str = ""
    vendor_hash: Optional[str] = None
    cargo_hash: Optional[str] = None
    npm_deps_hash: Optional[str] = None
    # Track where in the file each field is located
    version_line: int = 0
    hash_line: int = 0
    vendor_hash_line: int = 0
    cargo_hash_line: int = 0
    npm_deps_hash_line: int = 0
    # Raw text patterns for replacement
    raw_version_pattern: str = ""
    raw_hash_pattern: str = ""
    # Package type
    package_type: str = "standard"  # standard, go, rust, npm
    # Commit-based versioning (no tags/releases)
    uses_commit: bool = False  # True if rev = "full-commit-hash" pattern
    full_rev: str = ""  # Full commit hash if uses_commit
    hash_format: str = "sri"  # "sri" (sha256-...) or "base32" (old format)


class OverlayParser:
    """Parse Nix overlay files to extract package information."""

    # Overlay files to skip entirely (not auto-updatable)
    SKIP_FILES = {
        '00-last-known-good.nix',  # Packages pinned to specific nixpkgs revisions
    }

    # Package names to skip when using --all (not updatable or special)
    SKIP_PACKAGES = {
        'pythonPackagesExtensions',  # Not a package, extends python packages
        'coqPackages', 'coq',  # References to other packages
        'coqPackages_HEAD', 'coqPackages_9_1', 'coqPackages_9_0',
        'coqPackages_8_20', 'coqPackages_8_19', 'coqPackages_8_18',
        'coqPackages_8_17', 'coqPackages_8_16', 'coqPackages_8_15',
        'coqPackages_8_14', 'coqPackages_8_13', 'coqPackages_8_12',
        'coqPackages_8_11', 'coqPackages_8_10',
        'coq_HEAD', 'coq_9_1', 'coq_9_0', 'coq_8_20', 'coq_8_19',
        'coq_8_18', 'coq_8_17', 'coq_8_16', 'coq_8_15', 'coq_8_14',
        'coq_8_13', 'coq_8_12', 'coq_8_11', 'coq_8_10',
        'ledger_HEAD',  # Local build from source
        'mcp-server-sequential-thinking',  # Override only, no version
    }

    def __init__(self, overlays_dir: Path):
        self.overlays_dir = overlays_dir

    def find_all_packages(self) -> list[str]:
        """Find all package names defined in overlay files."""
        packages = []
        for overlay_file in self.overlays_dir.glob("*.nix"):
            if overlay_file.name in self.SKIP_FILES:
                continue
            packages.extend(self._find_packages_in_file(overlay_file))
        return sorted(set(packages) - self.SKIP_PACKAGES)

    def _find_packages_in_file(self, file_path: Path) -> list[str]:
        """Find all package names defined in a single overlay file."""
        content = file_path.read_text()
        packages = []

        # Match top-level package definitions: name = ...
        # These are inside the `final: prev: { ... }` block, typically indented 2 spaces
        for line in content.split('\n'):
            # Skip empty lines and comments
            stripped = line.strip()
            if not stripped or stripped.startswith('#') or stripped.startswith('//'):
                continue
            # Match package definitions at top-level (column 0) or standard
            # overlay indentation (2 spaces inside final: prev: { })
            indent = len(line) - len(line.lstrip())
            if indent <= 2:
                match = re.match(r'^([a-zA-Z_][a-zA-Z0-9_-]*)\s*=', stripped)
                if match:
                    name = match.group(1)
                    # Skip 'self', 'super', 'pkgs', 'let', 'in', etc.
                    if name not in ('self', 'super', 'pkgs', 'let', 'in', 'with', 'inherit'):
                        packages.append(name)

        # Also find packages inside rec { } blocks (e.g., Emacs packages)
        # Look for fetchFromGitHub blocks and extract the enclosing package name
        packages.extend(self._find_nested_packages(content))

        return packages

    def _find_nested_packages(self, content: str) -> list[str]:
        """Find packages defined inside rec { } blocks with fetchFromGitHub."""
        packages = []
        lines = content.split('\n')

        # Track if we're inside a rec { } block
        in_rec_block = False
        brace_depth = 0

        for i, line in enumerate(lines):
            stripped = line.strip()

            # Detect rec { or in rec {
            if re.search(r'\brec\s*\{', line):
                in_rec_block = True

            # Count braces to track depth
            brace_depth += line.count('{') - line.count('}')

            if brace_depth <= 0:
                in_rec_block = False

            # Inside rec block, look for package definitions with fetchFromGitHub
            if in_rec_block and line.startswith('    '):  # Must be indented
                # Match: "package-name = compileEmacsFiles {" or similar on the stripped line
                match = re.match(r'^([a-zA-Z_][a-zA-Z0-9_-]*)\s*=\s*(?:compileEmacsFiles|compileEmacsWikiFile|mkDerivation)', stripped)
                if match:
                    name = match.group(1)
                    # Check if this package has fetchFromGitHub in the next ~20 lines
                    block_text = '\n'.join(lines[i:i+30])
                    if 'fetchFromGitHub' in block_text:
                        packages.append(name)

        return packages

    def find_package(self, package_name: str) -> Optional[PackageInfo]:
        """Find a package definition in overlay files."""
        for overlay_file in self.overlays_dir.glob("*.nix"):
            if overlay_file.name in self.SKIP_FILES:
                continue
            result = self._parse_file_for_package(overlay_file, package_name)
            if result:
                return result
        return None

    def _parse_file_for_package(self, file_path: Path, package_name: str) -> Optional[PackageInfo]:
        """Parse a single overlay file for a package definition."""
        content = file_path.read_text()

        # Pattern 1: Standard package definition (name = ... { })
        # Pattern 2: overrideAttrs pattern (name = prev.name.overrideAttrs...)
        # Pattern 3: Nested package (indented, inside rec { } block)

        # First, find the package definition block
        patterns = [
            # Standard: llama-swap = { ... } or llama-swap = with super; { ... }
            # Allow up to 2 spaces for overlay indentation (inside final: prev: { })
            rf'^[ ]{{0,2}}{re.escape(package_name)}\s*=',
            # overrideAttrs: llama-cpp = super.llama-cpp.overrideAttrs
            rf'^[ ]{{0,2}}{re.escape(package_name)}\s*=\s*(?:super|prev|pkgs)\.[a-zA-Z0-9_-]+\.overrideAttrs',
            # Nested (indented): gnus-harvest = compileEmacsFiles {
            rf'^\s+{re.escape(package_name)}\s*=\s*(?:compileEmacsFiles|compileEmacsWikiFile|mkDerivation)',
        ]

        for line_num, line in enumerate(content.split('\n'), 1):
            for pattern in patterns:
                if re.match(pattern, line):
                    return self._extract_package_info(content, file_path, package_name, line_num)

        return None

    def _extract_package_block(self, content: str, package_name: str) -> Optional[str]:
        """Extract just the block of code defining a specific package."""
        lines = content.split('\n')

        # Find the line where the package definition starts
        start_idx = None
        start_indent = 0  # Track the indentation level of the package
        for i, line in enumerate(lines):
            # Match: "package_name =" or "package_name=" (at any indentation)
            match = re.match(rf'^(\s*){re.escape(package_name)}\s*=', line)
            if match:
                start_idx = i
                start_indent = len(match.group(1))
                break

        if start_idx is None:
            return None

        # Find the end by looking for the next definition at same or lower indentation
        block_lines = [lines[start_idx]]

        # Track brace depth to understand nesting
        brace_depth = lines[start_idx].count('{') - lines[start_idx].count('}')

        for i in range(start_idx + 1, len(lines)):
            line = lines[i]

            # Update brace depth
            brace_depth += line.count('{') - line.count('}')

            # Check if we've reached a new definition at same or lower indentation
            stripped = line.lstrip()
            if stripped:
                current_indent = len(line) - len(stripped)

                # If brace depth is 0 or negative, and we find a new definition
                # at same/lower indent, we're done
                if brace_depth <= 0 and current_indent <= start_indent:
                    if re.match(r'^[a-zA-Z_][a-zA-Z0-9_-]*\s*=', stripped):
                        break
                    # Also stop at closing brace at same/lower indent
                    if stripped.startswith('}') and current_indent <= start_indent:
                        break

            block_lines.append(line)

            # Safety limit
            if len(block_lines) > 500:
                break

        return '\n'.join(block_lines)

    def _extract_package_info(self, content: str, file_path: Path, package_name: str, start_line: int) -> Optional[PackageInfo]:
        """Extract package information from the content."""
        info = PackageInfo(name=package_name, file_path=file_path)

        # Extract just the package block, not the whole file
        block = self._extract_package_block(content, package_name)
        if not block:
            return None

        # Extract GitHub owner/repo from the block
        owner_match = re.search(r'owner\s*=\s*"([^"]+)"', block)
        repo_match = re.search(r'repo\s*=\s*"([^"]+)"', block)

        if not owner_match or not repo_match:
            return None

        info.owner = owner_match.group(1)
        info.repo = repo_match.group(1)

        # Extract version - multiple patterns
        version_patterns = [
            # tag = "v${version}";
            (r'tag\s*=\s*"v\$\{version\}"', "v", False),
            # tag = "b${version}";
            (r'tag\s*=\s*"b\$\{version\}"', "b", False),
            # tag = version;
            (r'tag\s*=\s*version', "", False),
            # tag = "${version}";
            (r'tag\s*=\s*"\$\{version\}"', "", False),
            # rev = "v${version}";
            (r'rev\s*=\s*"v\$\{version\}"', "v", False),
            # rev = "${version}";
            (r'rev\s*=\s*"\$\{version\}"', "", False),
            # version = "123";
            (r'version\s*=\s*"([^"]+)"', "", False),
        ]

        for pattern, prefix, is_commit in version_patterns:
            match = re.search(pattern, block)
            if match:
                info.version_prefix = prefix
                info.uses_commit = is_commit
                break

        # Check for commit-based versioning: rev = "full-40-char-hash"
        # This is when rev is a literal commit hash, not interpolated
        rev_literal_match = re.search(r'rev\s*=\s*"([a-f0-9]{40})"', block)
        if rev_literal_match:
            info.uses_commit = True
            info.full_rev = rev_literal_match.group(1)

        # Now find the actual version value
        version_match = re.search(r'\bversion\s*=\s*"([^"$]+)"', block)
        if version_match:
            info.version = version_match.group(1)
            info.raw_version_pattern = version_match.group(0)

        # Extract source hash (SRI format: sha256-... or base32 format: old nix hash)
        sri_match = re.search(r'hash\s*=\s*"(sha256-[^"]+)"', block)
        base32_match = re.search(r'sha256\s*=\s*"([^"]+)"', block)

        if sri_match:
            info.src_hash = sri_match.group(1)
            info.raw_hash_pattern = sri_match.group(0)
            info.hash_format = "sri"
        elif base32_match:
            info.src_hash = base32_match.group(1)
            info.raw_hash_pattern = base32_match.group(0)
            info.hash_format = "base32"

        # Extract ALL hashes found in the block (packages can have multiple)
        # e.g., llama-swap has vendorHash (main) + npmDepsHash (ui subpackage)

        vendor_match = re.search(r'vendorHash\s*=\s*"(sha256-[^"]+)"', block)
        cargo_match = re.search(r'cargoHash\s*=\s*"(sha256-[^"]+)"', block)
        npm_match = re.search(r'npmDepsHash\s*=\s*"(sha256-[^"]+)"', block)

        if vendor_match:
            info.vendor_hash = vendor_match.group(1)
        if cargo_match:
            info.cargo_hash = cargo_match.group(1)
        if npm_match:
            info.npm_deps_hash = npm_match.group(1)

        # Set primary package type for display purposes
        # Priority: vendorHash (Go) > cargoHash (Rust) > npmDepsHash (npm)
        if vendor_match or 'buildGoModule' in block:
            info.package_type = "go"
        elif cargo_match or 'buildRustPackage' in block:
            info.package_type = "rust"
        elif npm_match or 'buildNpmPackage' in block:
            info.package_type = "npm"

        return info


class GitHubClient:
    """Client for GitHub API operations using gh CLI."""

    def get_latest_release(self, owner: str, repo: str) -> Optional[str]:
        """Get the latest release version from GitHub."""
        try:
            # Try latest release first
            result = subprocess.run(
                ["gh", "api", f"repos/{owner}/{repo}/releases/latest"],
                capture_output=True, text=True
            )
            if result.returncode == 0:
                data = json.loads(result.stdout)
                tag = data.get("tag_name", "")
                return tag
        except Exception:
            pass

        # Fall back to tags if no release
        try:
            result = subprocess.run(
                ["gh", "api", f"repos/{owner}/{repo}/tags", "--jq", ".[0].name"],
                capture_output=True, text=True
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip()
        except Exception:
            pass

        return None

    def get_latest_commit(self, owner: str, repo: str, branch: str = "master") -> Optional[tuple[str, str]]:
        """Get the latest commit SHA and short SHA from a branch. Returns (full_sha, short_sha)."""
        # Try master first, then main
        for br in [branch, "master", "main"]:
            try:
                result = subprocess.run(
                    ["gh", "api", f"repos/{owner}/{repo}/commits/{br}", "--jq", ".sha"],
                    capture_output=True, text=True
                )
                if result.returncode == 0 and result.stdout.strip():
                    full_sha = result.stdout.strip()
                    short_sha = full_sha[:8]
                    return (full_sha, short_sha)
            except Exception:
                pass
        return None


class HashComputer:
    """Compute Nix hashes for various package types."""

    def __init__(self, nix_dir: Path):
        self.nix_dir = nix_dir

    def compute_src_hash(self, owner: str, repo: str, rev: str) -> Optional[str]:
        """Compute source hash using nix-prefetch-github."""
        try:
            result = subprocess.run(
                ["nix", "run", "nixpkgs#nix-prefetch-github", "--",
                 owner, repo, "--rev", rev, "--json"],
                capture_output=True, text=True, timeout=300
            )
            if result.returncode == 0:
                data = json.loads(result.stdout)
                # nix-prefetch-github returns hash in SRI format
                return data.get("hash", data.get("sha256"))
        except Exception as e:
            print(f"  Error computing src hash: {e}", file=sys.stderr)
        return None

    def compute_vendor_hash(self, package_name: str) -> Optional[str]:
        """Compute vendorHash by attempting a build with fake hash."""
        return self._compute_fod_hash(package_name, "vendorHash")

    def compute_cargo_hash(self, package_name: str) -> Optional[str]:
        """Compute cargoHash by attempting a build with fake hash."""
        return self._compute_fod_hash(package_name, "cargoHash")

    def compute_npm_deps_hash(self, package_name: str) -> Optional[str]:
        """Compute npmDepsHash by attempting a build with fake hash."""
        return self._compute_fod_hash(package_name, "npmDepsHash")

    def _compute_fod_hash(self, package_name: str, hash_type: str) -> Optional[str]:
        """Compute a fixed-output derivation hash by parsing build failure."""
        print(f"  Computing {hash_type} (this may take a while)...")

        # Build the package and capture the error with the correct hash
        try:
            result = subprocess.run(
                ["./build", "pkg", package_name],
                capture_output=True, text=True, timeout=600,
                cwd=self.nix_dir
            )

            # Even if build succeeds, we might need the hash from output
            output = result.stdout + result.stderr

            # Look for the hash mismatch error
            # Pattern: "got: sha256-..."
            hash_match = re.search(r'got:\s+(sha256-[A-Za-z0-9+/=]+)', output)
            if hash_match:
                return hash_match.group(1)

            # Also try: "got:    sha256-..."
            hash_match = re.search(r'got:\s+sha256:([A-Za-z0-9+/=]+)', output)
            if hash_match:
                # Convert to SRI format
                return f"sha256-{hash_match.group(1)}"

        except subprocess.TimeoutExpired:
            print(f"  Timeout computing {hash_type}", file=sys.stderr)
        except Exception as e:
            print(f"  Error computing {hash_type}: {e}", file=sys.stderr)

        return None


class OverlayUpdater:
    """Update overlay files with new versions and hashes."""

    def set_dummy_hash(self, file_path: Path, package_name: str, hash_type: str, old_hash: str) -> bool:
        """Set a dummy hash for a specific package to trigger hash computation on build."""
        content = file_path.read_text()
        lines = content.split('\n')

        start_idx, end_idx = self._find_package_block_lines(content, package_name)
        if start_idx < 0:
            return False

        # Extract and update the block
        block_lines = lines[start_idx:end_idx + 1]
        block = '\n'.join(block_lines)

        # Map hash type to pattern
        patterns = {
            'vendorHash': rf'(vendorHash\s*=\s*")({re.escape(old_hash)})(")',
            'cargoHash': rf'(cargoHash\s*=\s*")({re.escape(old_hash)})(")',
            'npmDepsHash': rf'(npmDepsHash\s*=\s*")({re.escape(old_hash)})(")',
        }

        pattern = patterns.get(hash_type)
        if not pattern:
            return False

        dummy_hash = "sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
        new_block = re.sub(pattern, rf'\g<1>{dummy_hash}\g<3>', block)

        if new_block == block:
            return False

        # Reconstruct file
        new_lines = lines[:start_idx] + new_block.split('\n') + lines[end_idx + 1:]
        file_path.write_text('\n'.join(new_lines))
        return True

    def _find_package_block_lines(self, content: str, package_name: str) -> tuple[int, int]:
        """Find the start and end line indices for a package block."""
        lines = content.split('\n')

        # Find start (at any indentation level)
        start_idx = None
        start_indent = 0
        for i, line in enumerate(lines):
            match = re.match(rf'^(\s*){re.escape(package_name)}\s*=', line)
            if match:
                start_idx = i
                start_indent = len(match.group(1))
                break

        if start_idx is None:
            return (-1, -1)

        # Find end by tracking brace depth
        end_idx = start_idx
        brace_depth = lines[start_idx].count('{') - lines[start_idx].count('}')

        for i in range(start_idx + 1, len(lines)):
            line = lines[i]
            brace_depth += line.count('{') - line.count('}')

            stripped = line.lstrip()
            if stripped:
                current_indent = len(line) - len(stripped)

                # If brace depth is 0 or negative, and we find a new definition
                # at same/lower indent, we're done
                if brace_depth <= 0 and current_indent <= start_indent:
                    if re.match(r'^[a-zA-Z_][a-zA-Z0-9_-]*\s*=', stripped):
                        end_idx = i - 1
                        break
                    if stripped.startswith('}') and current_indent <= start_indent:
                        end_idx = i - 1
                        break

            end_idx = i

        return (start_idx, end_idx)

    def update_package(self, info: PackageInfo, new_version: str, new_src_hash: str,
                      new_vendor_hash: Optional[str] = None,
                      new_cargo_hash: Optional[str] = None,
                      new_npm_deps_hash: Optional[str] = None,
                      new_full_rev: Optional[str] = None,
                      dry_run: bool = False) -> bool:
        """Update a package in its overlay file."""
        content = info.file_path.read_text()
        lines = content.split('\n')

        # Find the package block boundaries
        start_idx, end_idx = self._find_package_block_lines(content, info.name)
        if start_idx < 0:
            print(f"  Could not find package block for {info.name}")
            return False

        # Extract the block
        block_lines = lines[start_idx:end_idx + 1]
        block = '\n'.join(block_lines)
        original_block = block

        # Update version within the block
        old_version = info.version
        block = re.sub(
            rf'(\bversion\s*=\s*")({re.escape(old_version)})(")',
            rf'\g<1>{new_version}\g<3>',
            block
        )

        # For commit-based packages, also update the rev
        if info.uses_commit and new_full_rev and info.full_rev:
            block = re.sub(
                rf'(rev\s*=\s*")({re.escape(info.full_rev)})(")',
                rf'\g<1>{new_full_rev}\g<3>',
                block
            )

        # Update source hash within the block
        if new_src_hash and info.src_hash:
            if info.hash_format == "sri":
                block = re.sub(
                    rf'(hash\s*=\s*")({re.escape(info.src_hash)})(")',
                    rf'\g<1>{new_src_hash}\g<3>',
                    block
                )
            else:
                # base32 format - update sha256 = "..."
                block = re.sub(
                    rf'(sha256\s*=\s*")({re.escape(info.src_hash)})(")',
                    rf'\g<1>{new_src_hash}\g<3>',
                    block
                )

        # Update vendorHash if present
        if new_vendor_hash and info.vendor_hash:
            block = re.sub(
                rf'(vendorHash\s*=\s*")({re.escape(info.vendor_hash)})(")',
                rf'\g<1>{new_vendor_hash}\g<3>',
                block
            )

        # Update cargoHash if present
        if new_cargo_hash and info.cargo_hash:
            block = re.sub(
                rf'(cargoHash\s*=\s*")({re.escape(info.cargo_hash)})(")',
                rf'\g<1>{new_cargo_hash}\g<3>',
                block
            )

        # Update npmDepsHash if present
        if new_npm_deps_hash and info.npm_deps_hash:
            block = re.sub(
                rf'(npmDepsHash\s*=\s*")({re.escape(info.npm_deps_hash)})(")',
                rf'\g<1>{new_npm_deps_hash}\g<3>',
                block
            )

        if block == original_block:
            return False

        if dry_run:
            return True

        # Reconstruct the file with the updated block
        new_lines = lines[:start_idx] + block.split('\n') + lines[end_idx + 1:]
        new_content = '\n'.join(new_lines)

        # Write the updated content
        info.file_path.write_text(new_content)
        return True


def extract_version_number(tag: str, prefix: str = "") -> str:
    """Extract version number from a tag, removing common prefixes."""
    version = tag

    # Remove common prefixes
    for p in ["v", "b", "release-", "version-"]:
        if version.startswith(p):
            version = version[len(p):]
            break

    return version


def main():
    parser = argparse.ArgumentParser(
        description="Update Nix overlay package definitions to their latest versions",
        epilog="""
Examples:
  update-overlay llama-cpp gguf-tools    Update to latest release/commit
  update-overlay --dry-run llama-swap    Preview changes without modifying
  update-overlay --no-build hfdownloader Skip slow hash computation (src only)
  update-overlay -V 1.2.3 browser-control-mcp  Force specific version
  update-overlay --all --dry-run         Preview updates for all packages
  update-overlay --all --no-build        Update all packages (src hash only)
""",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "packages", nargs="*",
        help="Package names to update (not required if --all is used)"
    )
    parser.add_argument(
        "--all", "-a", action="store_true",
        help="Update all packages found in overlay files"
    )
    parser.add_argument(
        "--dry-run", "-n", action="store_true",
        help="Show what would be changed without making changes"
    )
    parser.add_argument(
        "--overlays-dir", "-d", type=Path,
        default=Path(__file__).parent.parent / "overlays",
        help="Directory containing overlay files"
    )
    parser.add_argument(
        "--no-build", action="store_true",
        help="Skip computing vendorHash/cargoHash/npmDepsHash (faster but incomplete)"
    )
    parser.add_argument(
        "--version", "-V",
        help="Specify version instead of fetching latest"
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true",
        help="Show skipped packages (no GitHub source)"
    )

    args = parser.parse_args()

    # Validate arguments
    if not args.all and not args.packages:
        parser.error("Please specify package names or use --all")

    if args.all and args.version:
        parser.error("Cannot use --version with --all")

    nix_dir = Path(__file__).parent.parent.resolve()
    overlay_parser = OverlayParser(args.overlays_dir)
    github_client = GitHubClient()
    hash_computer = HashComputer(nix_dir)
    updater = OverlayUpdater()

    # Determine which packages to process
    if args.all:
        packages = overlay_parser.find_all_packages()
        print(f"Found {len(packages)} packages in overlay files")
        if args.packages:
            # If both --all and package names given, filter to just those
            packages = [p for p in packages if p in args.packages]
            print(f"Filtered to {len(packages)} specified packages")
    else:
        packages = args.packages

    success_count = 0
    fail_count = 0
    skipped_count = 0

    for package_name in packages:
        # Find package in overlays
        info = overlay_parser.find_package(package_name)
        if not info:
            if args.all:
                # In --all mode, silently skip packages without GitHub info
                if args.verbose:
                    print(f"{Colors.DIM}{package_name}: skipped (no GitHub source){Colors.RESET}")
                skipped_count += 1
            else:
                print(f"{Colors.RED}{package_name}: not found or no GitHub source{Colors.RESET}")
                fail_count += 1
            continue

        # Build hash info string
        hash_types = []
        if info.src_hash:
            hash_types.append("src")
        if info.vendor_hash:
            hash_types.append("vendor")
        if info.cargo_hash:
            hash_types.append("cargo")
        if info.npm_deps_hash:
            hash_types.append("npm")
        hash_info = f" [{','.join(hash_types)}]" if len(hash_types) > 1 else ""

        # Compact package info
        pkg_display = f"{Colors.BOLD}{package_name}{Colors.RESET}"
        version_display = f"{Colors.DIM}{info.version}{Colors.RESET}"
        print(f"{pkg_display} {version_display}{hash_info}", end=" ")

        # Get latest version
        latest_tag = None
        new_version = None
        new_full_rev = None

        if args.version:
            latest_tag = args.version
            if not latest_tag.startswith(("v", "b")):
                latest_tag = f"{info.version_prefix}{args.version}"
            new_version = extract_version_number(latest_tag, info.version_prefix)
        elif info.uses_commit:
            # Package uses commit-based versioning, fetch latest commit
            commit_info = github_client.get_latest_commit(info.owner, info.repo)
            if not commit_info:
                print(f"{Colors.RED}✗ fetch failed{Colors.RESET}")
                fail_count += 1
                continue
            new_full_rev, new_version = commit_info
            latest_tag = new_full_rev  # Use full rev for hash computation
        else:
            latest_tag = github_client.get_latest_release(info.owner, info.repo)
            if not latest_tag:
                # Try falling back to commits
                commit_info = github_client.get_latest_commit(info.owner, info.repo)
                if commit_info:
                    new_full_rev, new_version = commit_info
                    latest_tag = new_full_rev
                    info.uses_commit = True  # Switch to commit mode
                else:
                    print(f"{Colors.RED}✗ fetch failed{Colors.RESET}")
                    fail_count += 1
                    continue
            else:
                new_version = extract_version_number(latest_tag, info.version_prefix)

        if new_version == info.version:
            print(f"{Colors.GREEN}✓ up-to-date{Colors.RESET}")
            skipped_count += 1
            continue

        # Show update arrow
        print(f"{Colors.YELLOW}→ {new_version}{Colors.RESET}", end="")

        # Compute new source hash
        new_src_hash = hash_computer.compute_src_hash(info.owner, info.repo, latest_tag)
        if not new_src_hash:
            print(f" {Colors.RED}✗ hash failed{Colors.RESET}")
            fail_count += 1
            continue

        # Update the overlay file with new version and src hash first
        # This is needed so we can compute the other hashes
        temp_update = updater.update_package(
            info, new_version, new_src_hash,
            new_full_rev=new_full_rev,
            dry_run=args.dry_run
        )

        # Compute additional hashes if needed
        new_vendor_hash = None
        new_cargo_hash = None
        new_npm_deps_hash = None
        extra_hashes = []

        if not args.no_build and not args.dry_run:
            if info.vendor_hash:
                updater.set_dummy_hash(info.file_path, package_name, 'vendorHash', info.vendor_hash)
                new_vendor_hash = hash_computer.compute_vendor_hash(package_name)
                if new_vendor_hash:
                    extra_hashes.append("vendor")

            if info.cargo_hash:
                updater.set_dummy_hash(info.file_path, package_name, 'cargoHash', info.cargo_hash)
                new_cargo_hash = hash_computer.compute_cargo_hash(package_name)
                if new_cargo_hash:
                    extra_hashes.append("cargo")

            if info.npm_deps_hash:
                updater.set_dummy_hash(info.file_path, package_name, 'npmDepsHash', info.npm_deps_hash)
                new_npm_deps_hash = hash_computer.compute_npm_deps_hash(package_name)
                if new_npm_deps_hash:
                    extra_hashes.append("npm")

        # Final update with all hashes
        if not args.dry_run:
            info = overlay_parser.find_package(package_name)
            updater.update_package(
                info, new_version, new_src_hash,
                new_vendor_hash=new_vendor_hash,
                new_cargo_hash=new_cargo_hash,
                new_npm_deps_hash=new_npm_deps_hash,
                new_full_rev=new_full_rev,
                dry_run=False
            )

        success_count += 1
        extra_info = f" +{','.join(extra_hashes)}" if extra_hashes else ""
        if args.dry_run:
            print(f" {Colors.CYAN}(dry-run){Colors.RESET}")
        else:
            print(f" {Colors.GREEN}✓{Colors.RESET}{extra_info}")

    # Colored summary
    updated_str = f"{Colors.GREEN}{success_count} updated{Colors.RESET}" if success_count else f"{success_count} updated"
    skipped_str = f"{skipped_count} up-to-date"
    failed_str = f"{Colors.RED}{fail_count} failed{Colors.RESET}" if fail_count else f"{fail_count} failed"
    print(f"\n{Colors.BOLD}Summary:{Colors.RESET} {updated_str}, {skipped_str}, {failed_str}")
    return 0 if fail_count == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
